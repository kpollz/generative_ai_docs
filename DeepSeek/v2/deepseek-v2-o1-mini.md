## üêã DeepSeek-V2


### 1. DeepSeek-V2 l√† g√¨?

- **DeepSeek-V2** l√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (Large Language Model ‚Äì LLM) ti√™n ti·∫øn, ƒë∆∞·ª£c x√¢y d·ª±ng theo ki·∫øn tr√∫c **Mixture-of-Experts (MoE)**.
- T·ªïng s·ªë tham s·ªë c·ªßa m√¥ h√¨nh l√™n ƒë·∫øn **236 t·ª∑ (236B)**, nh∆∞ng ch·ªâ k√≠ch ho·∫°t **21 t·ª∑ (21B)** tham s·ªë cho m·ªói token trong qu√° tr√¨nh suy di·ªÖn.
- M√¥ h√¨nh n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi nh·ªØng c·∫£i ti·∫øn ƒë·ªôt ph√° nh·∫±m **gi·∫£m chi ph√≠ hu·∫•n luy·ªán** v√† **tƒÉng t·ªëc qu√° tr√¨nh suy di·ªÖn (inference)**, ƒë·ªìng th·ªùi h·ªó tr·ª£ x·ª≠ l√Ω c√°c ng·ªØ c·∫£nh d√†i l√™n ƒë·∫øn **128K token**.

---

### 2. M·ª•c ti√™u v√† lƒ©nh v·ª±c ·ª©ng d·ª•ng c·ªßa DeepSeek-V2

- **B√†i to√°n tr·ªçng t√¢m:** Khi c√°c m√¥ h√¨nh LLM ƒë∆∞·ª£c m·ªü r·ªông v·ªÅ s·ªë l∆∞·ª£ng tham s·ªë, hi·ªáu nƒÉng th∆∞·ªùng c·∫£i thi·ªán nh∆∞ng ƒëi k√®m v·ªõi chi ph√≠ t√≠nh to√°n l·ªõn (c·∫£ trong giai ƒëo·∫°n hu·∫•n luy·ªán l·∫´n suy di·ªÖn). ƒê·∫∑c bi·ªát, vi·ªác l∆∞u tr·ªØ v√† x·ª≠ l√Ω b·ªô nh·ªõ cache (Key-Value cache) trong qu√° tr√¨nh suy di·ªÖn l√† m·ªôt trong nh·ªØng n√∫t th·∫Øt l·ªõn.
- **DeepSeek-V2 ra ƒë·ªùi ƒë·ªÉ:**  
  - **Ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán:** B·∫±ng c√°ch ch·ªâ k√≠ch ho·∫°t m·ªôt ph·∫ßn nh·ªè c√°c tham s·ªë (21B tr√™n 236B t·ªïng s·ªë) v√† s·ª≠ d·ª•ng ki·∫øn tr√∫c MoE hi·ªáu qu·∫£.
  - **TƒÉng t·ªëc suy di·ªÖn:** Th√¥ng qua vi·ªác gi·∫£m ƒë√°ng k·ªÉ k√≠ch th∆∞·ªõc b·ªô nh·ªõ KV cache v√† t·ªëi ∆∞u c√°c ph√©p t√≠nh li√™n quan ƒë·∫øn attention.
  - **H·ªó tr·ª£ x·ª≠ l√Ω c√°c ng·ªØ c·∫£nh c·ª±c d√†i:** L√™n ƒë·∫øn 128K tokens, m·ªü r·ªông kh·∫£ nƒÉng ·ª©ng d·ª•ng trong c√°c t√°c v·ª• c·∫ßn l∆∞u gi·ªØ th√¥ng tin d√†i h·∫°n.

---

### 3. C√°c stage-of-the-art tr∆∞·ªõc DeepSeek-V2

Tr∆∞·ªõc DeepSeek-V2, ƒë√£ c√≥ m·ªôt s·ªë m√¥ h√¨nh v√† k·ªπ thu·∫≠t n·ªïi b·∫≠t nh·∫±m gi·∫£i quy·∫øt c√°c th√°ch th·ª©c trong hu·∫•n luy·ªán v√† suy di·ªÖn c·ªßa LLM:

- **C√°c m√¥ h√¨nh dense (ƒë·∫∑c):**  
  - **LLaMA, LLaMA 2, LLaMA 3:** Nh·ªØng m√¥ h√¨nh n√†y t·∫≠p trung v√†o vi·ªác tƒÉng s·ªë l∆∞·ª£ng tham s·ªë ƒë·ªÉ c·∫£i thi·ªán hi·ªáu nƒÉng, nh∆∞ng l·∫°i g·∫∑p h·∫°n ch·∫ø v·ªÅ chi ph√≠ t√≠nh to√°n v√† t·ªëc ƒë·ªô suy di·ªÖn.
  
- **C√°c m√¥ h√¨nh MoE kh√°c:**  
  - **DeepSeek 67B:** Phi√™n b·∫£n tr∆∞·ªõc c·ªßa DeepSeek v·ªõi ki·∫øn tr√∫c MoE, ƒë√£ cho th·∫•y kh·∫£ nƒÉng ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán so v·ªõi m√¥ h√¨nh dense.
  - **Mixtral, Qwen:** C√°c m√¥ h√¨nh MoE v√† hybrid kh√°c c≈©ng ƒë√£ th·ª≠ nghi·ªám ƒë·ªÉ t·ªëi ∆∞u chi ph√≠ v√† tƒÉng t·ªëc suy di·ªÖn.
  
- **C√°c c·∫£i ti·∫øn trong c∆° ch·∫ø attention:**  
  - **Multi-Head Attention (MHA):** Ph·ªï bi·∫øn trong h·∫ßu h·∫øt c√°c m√¥ h√¨nh Transformer nh∆∞ng l·∫°i c√≥ b·ªô nh·ªõ KV cache n·∫∑ng.
  - **Multi-Query Attention (MQA) v√† Grouped-Query Attention (GQA):** ƒê∆∞·ª£c ph√°t tri·ªÉn nh·∫±m gi·∫£m b·ªõt k√≠ch th∆∞·ªõc KV cache nh∆∞ng th∆∞·ªùng ƒë√°nh ƒë·ªïi ph·∫ßn n√†o v·ªÅ m·∫∑t hi·ªáu nƒÉng.

---

### 4. H·∫°n ch·∫ø c·ªßa SOTA tr∆∞·ªõc ƒë√≥ v√† ƒëi·ªÉm m·ªõi c·ªßa DeepSeek-V2

#### H·∫°n ch·∫ø c·ªßa c√°c ph∆∞∆°ng ph√°p SOTA:
- **Chi ph√≠ hu·∫•n luy·ªán cao:** M√¥ h√¨nh dense ƒë√≤i h·ªèi t√†i nguy√™n t√≠nh to√°n l·ªõn khi tƒÉng s·ªë l∆∞·ª£ng tham s·ªë.
- **Bottleneck ·ªü KV cache:** C√°c c∆° ch·∫ø attention nh∆∞ MHA c·∫ßn ph·∫£i l∆∞u tr·ªØ m·ªôt l∆∞·ª£ng l·ªõn d·ªØ li·ªáu (keys v√† values) cho t·ª´ng token, d·∫´n ƒë·∫øn hi·ªáu nƒÉng suy di·ªÖn th·∫•p khi x·ª≠ l√Ω ng·ªØ c·∫£nh d√†i.
- **Kh√≥ m·ªü r·ªông ng·ªØ c·∫£nh:** C√°c m√¥ h√¨nh tr∆∞·ªõc ƒë√¢y th∆∞·ªùng b·ªã gi·ªõi h·∫°n b·ªüi chi·ªÅu d√†i ng·ªØ c·∫£nh x·ª≠ l√Ω, kh√¥ng h·ªó tr·ª£ t·ªët c√°c t√°c v·ª• ƒë√≤i h·ªèi ng·ªØ c·∫£nh d√†i.

#### ƒêi·ªÉm m·ªõi v√† gi·∫£i ph√°p c·ªßa DeepSeek-V2:
- **Multi-head Latent Attention (MLA):**  
  - √Åp d·ª•ng k·ªπ thu·∫≠t **low-rank key-value joint compression** ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc KV cache xu·ªëng t·ªõi 93,3% so v·ªõi MHA truy·ªÅn th·ªëng.
  - K·∫øt h·ª£p v·ªõi chi·∫øn l∆∞·ª£c **decoupled Rotary Position Embedding (RoPE)** gi√∫p t√°ch bi·ªát ph·∫ßn ch·ªãu t√°c ƒë·ªông c·ªßa v·ªã tr√≠, t·ª´ ƒë√≥ t·ªëi ∆∞u h√≥a qu√° tr√¨nh suy di·ªÖn.
  
- **DeepSeekMoE:**  
  - Ki·∫øn tr√∫c MoE v·ªõi **ph√¢n ƒëo·∫°n chuy√™n m√¥n (fine-grained expert segmentation)** v√† c∆° ch·∫ø **chia s·∫ª expert (shared expert isolation)** gi√∫p m√¥ h√¨nh hu·∫•n luy·ªán v·ªõi chi ph√≠ th·∫•p h∆°n m√† v·∫´n ƒë·∫°t hi·ªáu nƒÉng cao.
  - C√°c c·∫£i ti·∫øn trong **device-limited routing** v√† c√°c h√†m m·∫•t m√°t ph·ª• tr·ª£ (auxiliary losses) ƒë·∫£m b·∫£o s·ª± c√¢n b·∫±ng t·∫£i v√† gi·∫£m thi·ªÉu overhead trong giao ti·∫øp gi·ªØa c√°c thi·∫øt b·ªã.
  
- **K·∫øt qu·∫£:**  
  - Ti·∫øt ki·ªám ƒë∆∞·ª£c kho·∫£ng **42,5% chi ph√≠ hu·∫•n luy·ªán** so v·ªõi phi√™n b·∫£n DeepSeek 67B.
  - T·ªëc ƒë·ªô suy di·ªÖn (throughput) ƒë∆∞·ª£c tƒÉng l√™n t·ªõi **5.76 l·∫ßn** so v·ªõi DeepSeek 67B.
  - H·ªó tr·ª£ context l√™n ƒë·∫øn 128K tokens, m·ªü r·ªông kh·∫£ nƒÉng ·ª©ng d·ª•ng trong c√°c t√°c v·ª• c·∫ßn l∆∞u gi·ªØ th√¥ng tin d√†i h·∫°n.

---

### 5. T·ªïng k·∫øt: C√°c c·∫£i ti·∫øn, h·∫°n ch·∫ø t·ªìn ƒë·ªçng v√† h∆∞·ªõng ph√°t tri·ªÉn trong t∆∞∆°ng lai

#### C√°c c·∫£i ti·∫øn n·ªïi b·∫≠t:
- **Ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán:** Ch·ªâ k√≠ch ho·∫°t 21B tham s·ªë trong t·ªïng s·ªë 236B, gi√∫p gi·∫£m chi ph√≠ t√≠nh to√°n m√† v·∫´n duy tr√¨ hi·ªáu nƒÉng cao.
- **TƒÉng t·ªëc suy di·ªÖn:** Nh·ªù v√†o ki·∫øn tr√∫c MLA gi√∫p gi·∫£m ƒë√°ng k·ªÉ k√≠ch th∆∞·ªõc KV cache v√† t·ªëi ∆∞u h√≥a qu√° tr√¨nh t√≠nh to√°n.
- **H·ªó tr·ª£ ng·ªØ c·∫£nh c·ª±c d√†i:** C√≥ th·ªÉ x·ª≠ l√Ω c√°c t√°c v·ª• v·ªõi context l√™n ƒë·∫øn 128K tokens, m·ªü r·ªông ph·∫°m vi ·ª©ng d·ª•ng.
- **Ki·∫øn tr√∫c MoE ti√™n ti·∫øn (DeepSeekMoE):** Gi√∫p hu·∫•n luy·ªán hi·ªáu qu·∫£ h∆°n v√† kh·∫£ nƒÉng chuy√™n m√¥n h√≥a c·ªßa c√°c ‚Äúexpert‚Äù ƒë∆∞·ª£c c·∫£i thi·ªán th√¥ng qua c√°c c∆° ch·∫ø routing v√† c√¢n b·∫±ng t·∫£i m·ªõi.

#### H·∫°n ch·∫ø c√≤n t·ªìn ƒë·ªçng:
- **ƒê·ªô ph·ª©c t·∫°p c·ªßa ki·∫øn tr√∫c:** Vi·ªác t√≠ch h·ª£p c√°c c∆° ch·∫ø MoE v√† c√°c c·∫£i ti·∫øn trong attention c√≥ th·ªÉ khi·∫øn qu√° tr√¨nh tri·ªÉn khai v√† b·∫£o tr√¨ tr·ªü n√™n ph·ª©c t·∫°p.
- **C√¢n b·∫±ng t·∫£i v√† giao ti·∫øp gi·ªØa c√°c expert:** D√π ƒë√£ c√≥ c√°c c∆° ch·∫ø gi·∫£m t·∫£i v√† c√¢n b·∫±ng, nh∆∞ng vi·ªác t·ªëi ∆∞u ho√° giao ti·∫øp gi·ªØa c√°c thi·∫øt b·ªã trong m√¥i tr∆∞·ªùng ph√¢n t√°n v·∫´n l√† m·ªôt th√°ch th·ª©c.
- **Kh·∫£ nƒÉng ·ª©ng d·ª•ng ƒëa ng√¥n ng·ªØ:** M·∫∑c d√π ƒë√£ h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Trung, nh∆∞ng c√≥ th·ªÉ c·∫ßn c·∫£i thi·ªán th√™m ƒë·ªÉ m·ªü r·ªông sang c√°c ng√¥n ng·ªØ kh√°c.

#### H∆∞·ªõng ph√°t tri·ªÉn trong t∆∞∆°ng lai:
- **T·ªëi ∆∞u h√≥a ki·∫øn tr√∫c MoE:** Nghi√™n c·ª©u th√™m c√°c k·ªπ thu·∫≠t m·ªõi ƒë·ªÉ c·∫£i thi·ªán s·ª± c√¢n b·∫±ng t·∫£i v√† gi·∫£m thi·ªÉu overhead giao ti·∫øp gi·ªØa c√°c expert.
- **M·ªü r·ªông kh·∫£ nƒÉng ƒëa ng√¥n ng·ªØ:** TƒÉng c∆∞·ªùng d·ªØ li·ªáu hu·∫•n luy·ªán v√† ƒëi·ªÅu ch·ªânh ki·∫øn tr√∫c ƒë·ªÉ ƒë√°p ·ª©ng t·ªët h∆°n cho c√°c ng√¥n ng·ªØ ngo√†i ti·∫øng Anh v√† ti·∫øng Trung.
- **C·∫£i ti·∫øn hi·ªáu nƒÉng suy di·ªÖn:** T√¨m ki·∫øm c√°c gi·∫£i ph√°p ph·∫ßn c·ª©ng v√† thu·∫≠t to√°n m·ªõi nh·∫±m t·ªëi ∆∞u ho√° qu√° tr√¨nh suy di·ªÖn, ƒë·∫∑c bi·ªát trong c√°c ·ª©ng d·ª•ng th·ªùi gian th·ª±c.
- **Nghi√™n c·ª©u h∆∞·ªõng h·ª£p nh·∫•t c√°c k·ªπ thu·∫≠t hi·ªán c√≥:** K·∫øt h·ª£p c√°c c·∫£i ti·∫øn t·ª´ MLA, DeepSeekMoE v·ªõi c√°c ti·∫øn b·ªô m·ªõi trong lƒ©nh v·ª±c h·ªçc s√¢u ƒë·ªÉ ti·∫øp t·ª•c n√¢ng cao hi·ªáu nƒÉng v√† kh·∫£ nƒÉng ·ª©ng d·ª•ng c·ªßa LLM.

---

## üîç APPENDIX

### 1. Gi·∫£i th√≠ch s∆° b·ªô v·ªÅ Multi-Query Attention (MQA) v√† Grouped-Query Attention (GQA)

Trong ki·∫øn tr√∫c Transformer, **Multi-Head Attention (MHA)** truy·ªÅn th·ªëng t√≠nh to√°n c√°c vector truy v·∫•n (query), kh√≥a (key) v√† gi√° tr·ªã (value) ri√™ng cho m·ªói ‚Äúƒë·∫ßu‚Äù (head). ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ƒë·ªëi v·ªõi m·ªói token, ta ph·∫£i l∆∞u tr·ªØ m·ªôt b·ªô key v√† value ri√™ng bi·ªát cho t·ª´ng head, d·∫´n ƒë·∫øn vi·ªác b·ªô nh·ªõ cache (KV cache) tr·ªü n√™n kh√° l·ªõn, ƒë·∫∑c bi·ªát l√† khi s·ªë l∆∞·ª£ng head tƒÉng l√™n.

**Multi-Query Attention (MQA):**
- **√ù t∆∞·ªüng ch√≠nh:** Thay v√¨ t√≠nh to√°n ri√™ng bi·ªát keys v√† values cho t·ª´ng head, MQA s·ª≠ d·ª•ng **m·ªôt b·ªô key v√† value chung cho t·∫•t c·∫£ c√°c head** trong attention.
- **∆Øu ƒëi·ªÉm:** Gi·∫£m ƒë√°ng k·ªÉ k√≠ch th∆∞·ªõc c·ªßa KV cache, v√¨ ch·ªâ c·∫ßn l∆∞u tr·ªØ m·ªôt b·ªô key v√† value duy nh·∫•t cho m·ªói token.
- **Nh∆∞·ª£c ƒëi·ªÉm:** Vi·ªác chia s·∫ª chung keys v√† values c√≥ th·ªÉ l√†m m·∫•t ƒëi t√≠nh ƒëa d·∫°ng m√† m·ªói head ri√™ng l·∫ª mang l·∫°i, d·∫´n ƒë·∫øn m·ªôt ch√∫t gi·∫£m hi·ªáu nƒÉng (kh·∫£ nƒÉng bi·ªÉu di·ªÖn th√¥ng tin c√≥ th·ªÉ kh√¥ng phong ph√∫ nh∆∞ khi c√≥ keys/values ri√™ng cho m·ªói head).

**Grouped-Query Attention (GQA):**
- **√ù t∆∞·ªüng ch√≠nh:** GQA l√† m·ªôt gi·∫£i ph√°p trung gian gi·ªØa MHA v√† MQA. ·ªû ƒë√¢y, c√°c head ƒë∆∞·ª£c **chia th√†nh c√°c nh√≥m** v√† trong m·ªói nh√≥m, c√°c head s·∫Ω chia s·∫ª m·ªôt b·ªô key v√† value chung.
- **∆Øu ƒëi·ªÉm:** So v·ªõi MHA, GQA v·∫´n gi·∫£m ƒë∆∞·ª£c k√≠ch th∆∞·ªõc c·ªßa KV cache (v√¨ kh√¥ng ph·∫£i m·ªói head ƒë·ªÅu c√≥ ri√™ng m·ªôt b·ªô key v√† value) nh∆∞ng ƒë·ªìng th·ªùi v·∫´n cho ph√©p duy tr√¨ m·ªôt ph·∫ßn t√≠nh ƒëa d·∫°ng b·∫±ng c√°ch kh√¥ng chia s·∫ª chung gi·ªØa t·∫•t c·∫£ c√°c head.
- **Nh∆∞·ª£c ƒëi·ªÉm:** M·∫∑c d√π gi·∫£m k√≠ch th∆∞·ªõc cache, vi·ªác chia s·∫ª n√†y v·∫´n c√≥ th·ªÉ l√†m m·∫•t ƒëi m·ªôt ph·∫ßn kh·∫£ nƒÉng m√¥ h√¨nh h√≥a c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa c√°c token so v·ªõi MHA truy·ªÅn th·ªëng ‚Äì do ƒë√≥, hi·ªáu nƒÉng c√≥ th·ªÉ kh√¥ng b·∫±ng MHA nh∆∞ng l·∫°i t·ªët h∆°n MQA.

T√≥m l·∫°i, c·∫£ MQA v√† GQA ƒë·ªÅu ƒë∆∞·ª£c ph√°t tri·ªÉn v·ªõi m·ª•c ti√™u **gi·∫£m b·ªõt b·ªô nh·ªõ l∆∞u tr·ªØ KV cache** trong qu√° tr√¨nh suy di·ªÖn, nh∆∞ng ƒëi·ªÅu n√†y c√≥ th·ªÉ ƒëi k√®m v·ªõi m·ªôt s·ª± ƒë√°nh ƒë·ªïi v·ªÅ m·∫∑t hi·ªáu nƒÉng bi·ªÉu di·ªÖn so v·ªõi c√°ch t√≠nh truy·ªÅn th·ªëng c·ªßa MHA, v·ªën cho m·ªói head m·ªôt b·ªô key v√† value ri√™ng bi·ªát.

---

### 2. So s√°nh chi ti·∫øt gi·ªØa LLM d·ª±a tr√™n MoE v√† Dense

Theo ki·∫øn th·ª©c hi·ªán nay, c√°c m√¥ h√¨nh LLM ch·ªß y·∫øu c√≥ hai lo·∫°i ki·∫øn tr√∫c:

- **Dense (ƒë·∫∑c):**  
  - T·∫•t c·∫£ c√°c tham s·ªë c·ªßa m√¥ h√¨nh ƒë·ªÅu ƒë∆∞·ª£c k√≠ch ho·∫°t v√† s·ª≠ d·ª•ng trong qu√° tr√¨nh hu·∫•n luy·ªán v√† suy di·ªÖn.
  - ∆Øu ƒëi·ªÉm: ƒê∆°n gi·∫£n, hi·ªáu nƒÉng ·ªïn ƒë·ªãnh v√† th∆∞·ªùng cho k·∫øt qu·∫£ bi·ªÉu di·ªÖn t·ªët.
  - Nh∆∞·ª£c ƒëi·ªÉm: Khi m√¥ h√¨nh c√≥ s·ªë l∆∞·ª£ng tham s·ªë r·∫•t l·ªõn, chi ph√≠ t√≠nh to√°n v√† b·ªô nh·ªõ c≈©ng tƒÉng ƒë√°ng k·ªÉ.

- **MoE (Mixture-of-Experts ‚Äì h·ªón h·ª£p chuy√™n gia):**  
  - M√¥ h√¨nh c√≥ r·∫•t nhi·ªÅu ‚Äúexpert‚Äù (nh√≥m con) nh∆∞ng ch·ªâ m·ªôt s·ªë nh·ªè ƒë∆∞·ª£c k√≠ch ho·∫°t cho m·ªói token. V√≠ d·ª•, m·ªôt m√¥ h√¨nh c√≥ t·ªïng 236B tham s·ªë nh∆∞ng ch·ªâ k√≠ch ho·∫°t kho·∫£ng 21B tham s·ªë cho m·ªói token.
  - ∆Øu ƒëi·ªÉm: Gi·∫£m chi ph√≠ t√≠nh to√°n v√† b·ªô nh·ªõ (v√¨ ch·ªâ m·ªôt ph·∫ßn nh·ªè tham s·ªë ƒë∆∞·ª£c s·ª≠ d·ª•ng m·ªói l·∫ßn) m√† v·∫´n duy tr√¨ ƒë∆∞·ª£c hi·ªáu nƒÉng cao nh·ªù v√†o kh·∫£ nƒÉng chuy√™n m√¥n h√≥a c·ªßa t·ª´ng expert.
  - Nh∆∞·ª£c ƒëi·ªÉm: Ki·∫øn tr√∫c ph·ª©c t·∫°p h∆°n, ƒë√≤i h·ªèi c√°c c∆° ch·∫ø routing v√† c√¢n b·∫±ng t·∫£i gi·ªØa c√°c expert; vi·ªác tri·ªÉn khai v√† t·ªëi ∆∞u h√≥a giao ti·∫øp gi·ªØa c√°c thi·∫øt b·ªã ph√¢n t√°n c√≥ th·ªÉ g·∫∑p th√°ch th·ª©c.

D∆∞·ªõi ƒë√¢y l√† m·ªôt b·∫£ng so s√°nh minh h·ªça gi·ªØa m·ªôt s·ªë m√¥ h√¨nh LLM d·ª±a tr√™n Dense v√† MoE:

| **Model**                | **Ki·ªÉu ki·∫øn tr√∫c** | **T·ªïng s·ªë tham s·ªë** | **Tham s·ªë k√≠ch ho·∫°t** | **Ghi ch√∫**                                      |
|--------------------------|--------------------|---------------------|-----------------------|--------------------------------------------------|
| **Dense Models**         |                    |                     |                       |                                                  |
| GPT-3                    | Dense              | 175B                | 175B                  | M√¥ h√¨nh dense truy·ªÅn th·ªëng                       |
| LLaMA 2                  | Dense              | 70B                 | 70B                   | Dense, ƒë∆∞·ª£c t·ªëi ∆∞u cho hi·ªáu nƒÉng t·ªët             |
| LLaMA 3                  | Dense              | 70B                 | 70B                   | Dense, phi√™n b·∫£n m·ªõi v·ªõi c·∫£i ti·∫øn                  |
| Qwen1.5                  | Dense              | 72B                 | 72B                   | Dense, h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ                          |
| DeepSeek 67B             | Dense              | 67B                 | 67B                   | Phi√™n b·∫£n dense c·ªßa DeepSeek (tr∆∞·ªõc DeepSeek-V2)   |
|                          |                    |                     |                       |                                                  |
| **MoE Models**           |                    |                     |                       |                                                  |
| Switch Transformer       | MoE                | ~1.6T               | Ch·ªâ k√≠ch ho·∫°t ~1/16 t·ªïng (sparse) | M√¥ h√¨nh MoE c·ªßa Google, v·ªõi s·ªë l∆∞·ª£ng expert r·∫•t l·ªõn    |
| GLaM                     | MoE                | ~1.2T               | Kho·∫£ng 100B (k√≠ch ho·∫°t th∆∞a) | M√¥ h√¨nh MoE c·ªßa Google, s·ª≠ d·ª•ng k√≠ch ho·∫°t th∆∞a          |
| Mixtral 8x22B            | MoE                | ~141B               | 39B                   | MoE, s·ª≠ d·ª•ng 8 expert, k√≠ch ho·∫°t m·ªôt s·ªë expert nh·∫•t ƒë·ªãnh|
| DeepSeek-V2              | MoE                | 236B                | 21B                   | MoE ti√™n ti·∫øn, t√≠ch h·ª£p c√°c c·∫£i ti·∫øn nh∆∞ MLA & DeepSeekMoE  |

---

### T√≥m t·∫Øt

- **Dense LLM:**  
  - S·ª≠ d·ª•ng to√†n b·ªô tham s·ªë cho m·ªói token.  
  - V√≠ d·ª•: GPT-3, LLaMA 2, LLaMA 3, Qwen1.5, DeepSeek 67B.

- **MoE LLM:**  
  - Ch·ªâ k√≠ch ho·∫°t m·ªôt ph·∫ßn nh·ªè c√°c expert (v·ªõi t·ªïng s·ªë tham s·ªë l·ªõn) cho m·ªói token, gi√∫p ti·∫øt ki·ªám chi ph√≠ t√≠nh to√°n v√† b·ªô nh·ªõ.  
  - V√≠ d·ª•: Switch Transformer, GLaM, Mixtral 8x22B, DeepSeek-V2.
